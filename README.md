# ğŸ§  Chat con IA usando Ollama  
### Proyecto educativo â€“ InteracciÃ³n Personaâ€“Ordenador I

Este repositorio contiene una **aplicaciÃ³n web de ejemplo** que implementa un **chat con Inteligencia Artificial** utilizando **Ollama** como backend de IA local y **Next.js** para la interfaz web.

El proyecto ha sido desarrollado con fines **docentes**, como material de apoyo para la asignatura **InteracciÃ³n Personaâ€“Ordenador I** del **Grado en IngenierÃ­a InformÃ¡tica**, con el objetivo de analizar y diseÃ±ar **interfaces conversacionales**.

---

## ğŸ¯ Objetivos del proyecto

- Implementar una **interfaz conversacional** basada en texto  
- Comprender el flujo de interacciÃ³n **usuario â†” sistema â†” IA**  
- Integrar un modelo de lenguaje en una aplicaciÃ³n web moderna  
- DiseÃ±ar una interfaz usable, clara y controlable  
- Aplicar principios de **diseÃ±o centrado en el usuario**  
- Utilizar IA **local y gratuita**, sin dependencia de APIs externas  

---

## ğŸ§© TecnologÃ­as utilizadas

- **Next.js (App Router)** â€“ frontend y backend  
- **React** y **TypeScript**  
- **Tailwind CSS** â€“ diseÃ±o de la interfaz  
- **Ollama** â€“ ejecuciÃ³n local de modelos de lenguaje (LLM)  
- **Modelo utilizado**: `llama3.1:8b`  
- **Streaming HTTP** para respuestas en tiempo real  

---

## ğŸ–¥ï¸ Arquitectura del sistema

```
Usuario
  â†“
Interfaz web (Next.js + React)
  â†“
API interna (/api/chat)
  â†“
Ollama (IA local)
  â†“
Modelo LLM (llama3.1)
```

---

## âš™ï¸ Requisitos del sistema

### Software
- Sistema operativo: macOS, Linux o Windows  
- Node.js: versiÃ³n **18 o superior**  
- npm (incluido con Node.js)  
- Ollama instalado y en ejecuciÃ³n  

### Hardware (recomendado)
- CPU moderna (Apple Silicon, Intel o AMD)  
- Memoria RAM: mÃ­nimo 8 GB (recomendado 16 GB)  
- GPU: opcional (Metal / CUDA / ROCm)  
- Espacio en disco: ~6 GB  

---

## âš™ï¸ InstalaciÃ³n de Ollama

### macOS (Homebrew)
```bash
brew install ollama
```

### Linux
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### Windows
Descargar desde: https://ollama.com

---

## ğŸš€ InstalaciÃ³n y ejecuciÃ³n del proyecto

### 1ï¸âƒ£ Clonar el repositorio
```bash
git clone https://github.com/TU_USUARIO/ollama-chat-ipo.git
cd ollama-chat-ipo
```

### 2ï¸âƒ£ Instalar dependencias
```bash
npm install
```

### 3ï¸âƒ£ Descargar el modelo
```bash
ollama pull llama3.1:8b
ollama list
```

### 4ï¸âƒ£ Crear archivo de entorno `.env.local`
```env
OLLAMA_HOST=http://127.0.0.1:11434
OLLAMA_MODEL=llama3.1:8b
```

âš ï¸ No subir este archivo a GitHub.

### 5ï¸âƒ£ Iniciar Ollama
```bash
ollama serve
```

### 6ï¸âƒ£ Ejecutar la aplicaciÃ³n
```bash
npm run dev
```

Abrir en el navegador:
```
http://localhost:3000
```

---

## âœ¨ Funcionalidades

- Chat conversacional con IA  
- Respuestas en streaming  
- Historial de conversaciones  
- Modo claro / oscuro  
- Control del usuario para detener respuestas  
- Persistencia local  

---

## ğŸ§  RelaciÃ³n con InteracciÃ³n Personaâ€“Ordenador

El proyecto permite analizar:
- Interfaces conversacionales  
- Feedback inmediato  
- Control del sistema  
- Persistencia del contexto  
- DiseÃ±o visual y usabilidad  

---

## ğŸ”’ Privacidad y coste

- EjecuciÃ³n 100% local  
- Sin envÃ­o de datos a terceros  
- Sin claves de API  
- Uso gratuito  

---

## ğŸ“š CrÃ©ditos

Proyecto desarrollado como **ejemplo educativo** para la asignatura:

**InteracciÃ³n Personaâ€“Ordenador I**  
**Grado en IngenierÃ­a InformÃ¡tica**

---

## ğŸ“ Licencia

Uso educativo y formativo.  
Reutilizable para aprendizaje y docencia.
