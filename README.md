ğŸ§  Chat con IA usando Ollama (Proyecto Educativo)

Este repositorio contiene una webapp de ejemplo que implementa un chat con Inteligencia Artificial utilizando Ollama como backend de IA local y Next.js para la interfaz web.

El objetivo principal del proyecto es aprender a diseÃ±ar e implementar interfaces conversacionales y comprender cÃ³mo se integra una IA en una aplicaciÃ³n interactiva, dentro de la asignatura InteracciÃ³n Personaâ€“Ordenador I del Grado en IngenierÃ­a InformÃ¡tica.

ğŸ¯ Objetivos del proyecto

Comprender el flujo de interacciÃ³n entre usuario e IA

Implementar un chat con streaming de respuestas

DiseÃ±ar una interfaz usable y clara

Separar correctamente frontend y backend

Usar IA local y gratuita, sin depender de APIs de pago

Analizar el chat como sistema interactivo

ğŸ§© TecnologÃ­as utilizadas

Next.js (App Router) â€“ frontend y backend

TypeScript

Tailwind CSS â€“ diseÃ±o de la interfaz

Ollama â€“ ejecuciÃ³n local de modelos de lenguaje (LLM)

Modelo: llama3.1:8b

Streaming HTTP (tokens en tiempo real)

ğŸ–¥ï¸ Arquitectura del sistema
Usuario
  â†“
Interfaz web (Next.js + React)
  â†“
API interna (/api/chat)
  â†“
Ollama (IA local)
  â†“
Modelo LLM (llama3.1)


La interfaz gestiona la interacciÃ³n

El backend actÃºa como proxy hacia Ollama

La IA se ejecuta localmente (privacidad y coste cero)

ğŸ“¸ Funcionalidades de la interfaz

Chat tipo ChatGPT

Streaming de respuestas (texto aparece progresivamente)

Historial de conversaciones

CreaciÃ³n y borrado de chats

Renombrado automÃ¡tico del chat

Modo claro / oscuro

Indicador â€œescribiendoâ€¦â€

BotÃ³n para detener la generaciÃ³n

Persistencia local (LocalStorage)

âš™ï¸ Requisitos previos
1ï¸âƒ£ Node.js

VersiÃ³n recomendada:

node >= 18

2ï¸âƒ£ Ollama

InstalaciÃ³n en macOS (Homebrew):

brew install ollama

ğŸš€ InstalaciÃ³n y ejecuciÃ³n
1ï¸âƒ£ Clonar el repositorio
git clone https://github.com/tu-usuario/ollama-chat-ipo.git
cd ollama-chat-ipo

2ï¸âƒ£ Instalar dependencias
npm install

3ï¸âƒ£ Descargar el modelo de IA
ollama pull llama3.1:8b


Comprueba que estÃ¡ instalado:

ollama list

4ï¸âƒ£ Crear el archivo de entorno

En la raÃ­z del proyecto:

touch .env.local


Contenido:

OLLAMA_HOST=http://127.0.0.1:11434
OLLAMA_MODEL=llama3.1:8b

5ï¸âƒ£ Iniciar Ollama

En una terminal:

ollama serve

6ï¸âƒ£ Ejecutar la aplicaciÃ³n

En otra terminal:

npm run dev


Abrir en el navegador:

http://localhost:3000


(o el puerto que indique la consola)

ğŸ“‚ Estructura del proyecto
app/
 â”œâ”€ api/
 â”‚   â””â”€ chat/
 â”‚       â””â”€ route.ts     # Backend (proxy a Ollama)
 â”œâ”€ page.tsx             # Interfaz del chat
 â””â”€ layout.tsx
public/
.env.local
package.json
README.md

ğŸ§ª Uso bÃ¡sico

Escribe un mensaje en el campo inferior

Pulsa Enter para enviar

La respuesta de la IA aparece progresivamente

Usa Stop para detener la generaciÃ³n

Crea nuevos chats desde la barra lateral

ğŸ§  RelaciÃ³n con InteracciÃ³n Personaâ€“Ordenador

Este proyecto permite trabajar conceptos clave de IPO:

Interfaces conversacionales

Feedback inmediato (streaming)

Control del usuario (detener respuesta)

Persistencia del contexto

DiseÃ±o centrado en el usuario

Carga cognitiva y claridad visual

Puede utilizarse como base para:

Evaluaciones heurÃ­sticas

Pruebas de usabilidad

RediseÃ±o de la interfaz

ComparaciÃ³n con otros tipos de interacciÃ³n

ğŸ”’ Privacidad y coste

No se envÃ­an datos a servicios externos

La IA se ejecuta 100% en local

No requiere claves de API

Uso completamente gratuito

ğŸ§© Posibles ampliaciones (trabajo futuro)

Selector de modelo

Soporte completo de Markdown

Voz (Speech-to-Text / Text-to-Speech)

EvaluaciÃ³n de usabilidad

MÃ©tricas de interacciÃ³n

Accesibilidad (WCAG)

ğŸ“š CrÃ©ditos

Proyecto desarrollado como ejemplo educativo para la asignatura
InteracciÃ³n Personaâ€“Ordenador I
Grado en IngenierÃ­a InformÃ¡tica

ğŸ“ Licencia

Este proyecto se distribuye con fines educativos.
Puedes modificarlo y reutilizarlo libremente para aprendizaje y docencia.